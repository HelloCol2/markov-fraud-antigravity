# markov-fraud-antigravity
Multi-agent reinforcement learning system modeling fraud detection as a two-player Markov game, featuring an adaptive Antigravity Defender that suppresses fraudster payoff and minimizes long-term system loss.

<div align="center">

# ğŸ§² Antigravity Defender

**Multi-Agent Reinforcement Learning for Adaptive Fraud Detection**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![Framework](https://img.shields.io/badge/Framework-PPO-green.svg)](https://stable-baselines3.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Research-orange.svg)]()
[![Game Theory](https://img.shields.io/badge/Game%20Theory-Nash%20Equilibrium-purple.svg)]()

*Learning to collapse adversarial profitability through adaptive counter-force*

[Features](#-key-features) â€¢ [Quick Start](#-quick-start) â€¢ [How It Works](#-how-it-works) â€¢ [Results](#-results) â€¢ [Documentation](#-documentation)

</div>

---

## ğŸ¯ What is Antigravity Defender?

Antigravity Defender is a **game-theoretic fraud detection system** that learns to suppress fraudulent behavior by treating fraud as a **strategic, adapting opponent** rather than random anomalies.

### The Problem with Traditional Systems

```diff
- Static Rules: "If risk > 0.7, flag" â†’ Easily exploited
- Always-Strict: 78% detection BUT 12.20 system loss (cost explosion)
- Adaptive Threshold: 30% fraud success, 11.30 system loss (reactive, not learning)
```

### The Antigravity Solution

```diff
+ Strategic Learning: Recognizes fraud as an adaptive adversary
+ Dynamic Counter-Force: Adjusts strictness based on adversarial profitability
+ Payoff Collapse: Makes fraud unprofitable through learned pressure
+ Nash Equilibrium: Balances security â†” cost â†” trust
```

**Result**: **17% fraud success** (vs 30% baseline) and **5.5 system loss** (vs 11.3 baseline)

---

## ğŸ§  The Antigravity Concept

```mermaid
graph LR
    A[Fraudster Strategy] -->|Exploits| B[Traditional Detector]
    B -->|Static Response| C[Fraudster Adapts]
    C -->|Cycle Repeats| A
    
    D[Fraudster Strategy] -->|Attempts| E[Antigravity Defender]
    E -->|Counter-Force| F[Payoff Collapses]
    F -->|Fraud Unprofitable| G[Nash Equilibrium]
    
    style E fill:#90EE90
    style G fill:#FFD700
```

Just as **antigravity** opposes gravitational pull, the **Antigravity Defender** applies **counter-force** against fraud's pull toward exploitation, achieving a balanced equilibrium.

---

## ğŸŒŸ Key Features

<table>
<tr>
<td width="50%">

### ğŸ® Two-Player Markov Game
- Fraudster vs Defender adversarial environment
- Strategic observation spaces (10 & 12 features)
- Action spaces: Attack intensity vs Detection strictness
- Adversarial reward functions

</td>
<td width="50%">

### ğŸ¤– Multi-Agent RL
- PPO-based learning agents
- Adversarial co-training (self-play)
- Nash equilibrium convergence
- 256â†’256â†’128 deep architecture

</td>
</tr>
<tr>
<td>

### ğŸ§² 5 Antigravity Principles
1. Strategic opponent recognition
2. Dynamic counter-force application
3. Long-term optimization (Î³=0.995)
4. Fraudster payoff collapse
5. Stable security-cost-trust equilibrium

</td>
<td>

### ğŸ“Š Comprehensive System
- 200k synthetic fraud transactions
- 6 baseline comparisons
- Real-time visualization tools
- Production-ready codebase

</td>
</tr>
</table>

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph Data Layer
        A[fraud_antigravity_synth-2.csv<br/>200k samples]
    end
    
    subgraph Environment Layer
        B[FraudAntigravityEnv<br/>Two-Player Markov Game]
    end
    
    subgraph Agent Layer
        C[Fraudster Agent<br/>PPO: 10â†’64â†’64â†’3]
        D[Antigravity Defender<br/>Enhanced PPO: 12â†’256â†’256â†’128â†’3]
    end
    
    subgraph Training Layer
        E[Phase 1: Pre-Training<br/>1000 episodes]
        F[Phase 2: Co-Training<br/>1000 episodes]
    end
    
    subgraph Evaluation Layer
        G[Baseline Comparison<br/>6 strategies Ã— 100 episodes]
        H[Metrics & Visualization<br/>Learning curves, heatmaps]
    end
    
    A --> B
    B --> C
    B --> D
    C --> E
    D --> E
    E --> F
    F --> G
    G --> H
    
    style A fill:#E3F2FD
    style B fill:#C8E6C9
    style D fill:#90EE90
    style H fill:#FFD700
```

---

## ğŸ”„ How It Works

### Single Transaction Flow

```mermaid
sequenceDiagram
    participant T as Transaction
    participant F as Fraudster
    participant E as Environment
    participant D as Defender
    
    T->>E: New transaction arrives<br/>(risk=0.65, amount=$850)
    E->>F: Observe [10 features]
    F->>F: Neural network decides
    F->>E: Action: Attack Type 2
    
    E->>D: Observe [12 features + signals]
    D->>D: Check antigravity principles:<br/>âœ“ Payoff trending up?<br/>âœ“ Fraud rate high?
    D->>E: Action: Strict Defense (2)
    
    E->>E: Execute:<br/>Detection score > threshold<br/>â†’ CAUGHT!
    
    E->>F: Reward: -0.42 (penalty)
    E->>D: Reward: -0.06 (cost)
    
    Note over F,D: Both agents learn from experience
```

### Training Pipeline (2000 Episodes)

```mermaid
graph TB
    Start([Start Training]) --> Init[Initialize Agents<br/>Random Weights]
    
    Init --> Phase1[Phase 1: Pre-Training<br/>Episodes 1-1000]
    Phase1 --> |Defender trains vs<br/>oracle fraudster| P1Learn[Learn Basic<br/>Counter-Strategies]
    
    P1Learn --> Phase2[Phase 2: Co-Training<br/>Episodes 1001-2000]
    
    Phase2 --> Round1[Round 1]
    Round1 --> |Train Fraudster 100 eps| FLearn1[Fraudster Exploits<br/>Defender Weaknesses]
    FLearn1 --> |Train Defender 100 eps| DLearn1[Defender Counters<br/>New Tactics]
    
    DLearn1 --> Eval1{Fraud Success<br/>38%}
    Eval1 --> Round2[Round 2-5<br/>Repeat]
    
    Round2 --> Converge[Nash Equilibrium<br/>Fraud Success: 17%]
    
    Converge --> Save[Save Models<br/>defender_final.zip]
    Save --> Evaluate[Evaluate vs<br/>6 Baselines]
    Evaluate --> Results([Results:<br/>17% fraud, 5.5 loss])
    
    style Phase1 fill:#E3F2FD
    style Phase2 fill:#C8E6C9
    style Converge fill:#90EE90
    style Results fill:#FFD700
```

---

## ğŸ§  Neural Network Architecture

### Antigravity Defender Network

```
Input Layer (12 neurons)              Hidden Layers (Deep)                Output Layer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Customer Risk      â”‚â”€â”€â”€â”€â”€â”                                           â”‚ Lenient (0)  â”‚
â”‚ Transaction Amount â”‚â”€â”€â”€â”€â”€â”¤                                           â”‚   5%         â”‚
â”‚ Time Context       â”‚â”€â”€â”€â”€â”€â”¤         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Recent Fraud Rate  â”‚â”€â”€â”€â”€â”€â”¤         â”‚ 256 neurons â”‚                   â”‚ Normal (1)   â”‚
â”‚ False Positive Rateâ”‚â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€>â”‚    ReLU     â”‚â”€â”€â”                â”‚   15%        â”‚
â”‚ Defense Budget     â”‚â”€â”€â”€â”€â”€â”¤         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Investigations     â”‚â”€â”€â”€â”€â”€â”¤                          â”‚                â”‚ Strict (2)   â”‚
â”‚ Fraudster Aggress  â”‚â”€â”€â”€â”€â”€â”¤         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   80%        â”‚
â”‚ Payoff Trend â­    â”‚â”€â”€â”€â”€â”€â”¤         â”‚ 256 neurons â”‚  â”œâ”€â”€>â”‚128 neuroâ”‚â”€>â”‚              â”‚
â”‚ System Loss        â”‚â”€â”€â”€â”€â”€â”¤         â”‚    ReLU     â”‚  â”‚   â”‚   ReLU  â”‚ â”‚  Softmax     â”‚
â”‚ System Stress      â”‚â”€â”€â”€â”€â”€â”¤         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Fraudster Budget   â”‚â”€â”€â”€â”€â”€â”˜                          â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚                      â”‚
                                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Value Head
                                                                        (State Value)
                                                       
Total Parameters: ~102,000
Why Deeper? Better strategic pattern recognition & long-term value estimation
```

---

## ğŸ¯ Antigravity Decision Algorithm

```python
def antigravity_decision(observation):
    """
    The 5 Principles in Action
    """
    # Extract strategic signals
    customer_risk = obs[0]
    fraud_rate = obs[3]
    fraudster_payoff_trend = obs[8]  # â­ Key signal
    fraudster_aggressiveness = obs[7]
    
    # PRINCIPLE 1 & 2: Strategic Recognition + Counter-Force
    if fraudster_payoff_trend > 0.3 and fraud_rate > 0.4:
        return STRICT  # Apply antigravity pressure!
    
    # PRINCIPLE 3: Efficiency over paranoia
    if fp_rate > 0.3 or defense_budget < 0.3:
        return LENIENT  # Preserve trust & efficiency
    
    # PRINCIPLE 5: Balanced threat assessment
    threat_score = (customer_risk + amount + 2*fraud_rate) / 4
    
    if threat_score > 0.65:
        return STRICT
    elif threat_score > 0.35:
        return NORMAL
    else:
        return LENIENT
    
    # PRINCIPLE 4: Result = Fraudster payoff collapses over time
```

---

## ğŸ“Š Results

### Performance Comparison (Real Data)

| Strategy | Fraud Success â†“ | System Loss â†“ | Detection â†‘ | F1 Score |
|----------|----------------|---------------|-------------|----------|
| **ğŸ§² Antigravity** | **17%** | **5.50** | **83%** | **0.85** |
| Adaptive Threshold | 30% | 11.30 | 70% | 0.73 |
| Always Strict | 21% | 12.20 | 79% | 0.71 |
| Static Threshold | 41% | 10.29 | 59% | 0.66 |
| Always Normal | 41% | 9.20 | 59% | 0.64 |
| Random | 41% | 9.53 | 59% | 0.58 |
| Always Lenient | 61% | 7.20 | 39% | 0.45 |

### Key Achievements

<table>
<tr>
<td align="center" width="25%">
<h3>ğŸ“‰ 43% Reduction</h3>
Fraud success vs best baseline<br/>
<b>30% â†’ 17%</b>
</td>
<td align="center" width="25%">
<h3>ğŸ’° 51% Lower Cost</h3>
System loss vs best baseline<br/>
<b>11.3 â†’ 5.5</b>
</td>
<td align="center" width="25%">
<h3>ğŸ¯ 61% Collapse</h3>
Fraudster payoff over training<br/>
<b>+0.35 â†’ +0.12</b>
</td>
<td align="center" width="25%">
<h3>âš–ï¸ Nash Equilibrium</h3>
Stable strategy convergence<br/>
<b>Episode 1800</b>
</td>
</tr>
</table>

### Learning Curves

The antigravity effect in action:

```
Episode Range    Fraud Success    Fraudster Payoff    Note
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0-500:               42%              +0.35          High fraud
500-1000:            31%              +0.22          â†“ Decreasing
1000-1500:           23%              +0.15          â†“ Counter-force
1500-2000:           17%              +0.12          âœ“ Collapsed!
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/antigravity-defender.git
cd antigravity-defender

# Install dependencies
pip install -r requirements.txt

# OR use quick setup
./setup.sh
```

### Generate Synthetic Data

```bash
python env/synth_data.py
# Output: env/fraud_antigravity_synth.csv (28MB, 200k samples)
```

### Train Antigravity Defender

```bash
# Quick test (10 episodes, ~2 minutes)
python training/train_antigravity_enhanced.py --test-mode

# Full training (2000 episodes, ~1-2 hours)
python training/train_antigravity_enhanced.py --episodes 2000
```

### Evaluate Performance

```bash
python training/evaluate.py \
    --defender-model checkpoints_enhanced/antigravity_defender_enhanced.zip \
    --episodes 100
```

### Visualize Results

```bash
python utils/visualize.py \
    --metrics checkpoints_enhanced/training_metrics.json \
    --output-dir figures/
```

---

## ğŸ“ Project Structure

```
antigravity-defender/
â”œâ”€â”€ ğŸ“Š env/
â”‚   â”œâ”€â”€ fraud_env.py              # Markov game environment
â”‚   â”œâ”€â”€ synth_data.py             # Synthetic data generator
â”‚   â””â”€â”€ fraud_antigravity_synth.csv
â”‚
â”œâ”€â”€ ğŸ¤– agents/
â”‚   â”œâ”€â”€ fraudster_agent.py        # Fraudster implementations
â”‚   â”œâ”€â”€ defender_agent.py         # Baseline defenders
â”‚   â””â”€â”€ antigravity_enhanced.py   # Enhanced strategic defender â­
â”‚
â”œâ”€â”€ ğŸ“ training/
â”‚   â”œâ”€â”€ train_marl.py             # Original training pipeline
â”‚   â”œâ”€â”€ train_antigravity_enhanced.py  # Enhanced training â­
â”‚   â””â”€â”€ evaluate.py               # Policy comparison
â”‚
â”œâ”€â”€ ğŸ”§ utils/
â”‚   â”œâ”€â”€ metrics.py                # Performance tracking
â”‚   â”œâ”€â”€ visualize.py              # Plotting tools
â”‚   â””â”€â”€ wrappers.py               # Gym environment wrappers
â”‚
â”œâ”€â”€ ğŸ“ docs/
â”‚   â”œâ”€â”€ ANTIGRAVITY_PRINCIPLES.md # Strategic philosophy
â”‚   â”œâ”€â”€ PROCESS_VISUALIZATION.md  # Algorithm flowcharts
â”‚   â”œâ”€â”€ COMPLETE_PROCESS_GUIDE.md # Navigation guide
â”‚   â””â”€â”€ medium_draft.md           # Publication draft
â”‚
â”œâ”€â”€ ğŸ“ˆ analysis/
â”‚   â”œâ”€â”€ baseline_analysis.py      # Baseline performance
â”‚   â”œâ”€â”€ REAL_OUTPUT_REPORT.md     # Data analysis results
â”‚   â””â”€â”€ DATA_ANALYSIS_REPORT.md   # Dataset statistics
â”‚
â””â”€â”€ ğŸ“‹ README.md                  # You are here!
```

---

## ğŸ“ The 5 Antigravity Principles

<details>
<summary><b>1ï¸âƒ£ Strategic Opponent Recognition</b></summary>

**Principle**: Fraud is a strategic, adapting adversaryâ€”not random noise.

**Implementation**: Observe `fraudster_payoff_trend` and `fraudster_aggressiveness` to detect strategic behavior evolution.

**Why It Matters**: Static rules assume fraud is random. Antigravity recognizes patterns and adapts.

</details>

<details>
<summary><b>2ï¸âƒ£ Dynamic Counter-Force Application</b></summary>

**Principle**: Increase strictness when fraud becomes profitable; relax when costs outweigh damage.

**Implementation**: If `payoff_trend > 0.3` AND `fraud_rate > 0.4` â†’ Apply STRICT defense.

**Why It Matters**: Disrupts exploitation at critical moments while conserving resources.

</details>

<details>
<summary><b>3ï¸âƒ£ Long-Term Optimization</b></summary>

**Principle**: Maximize cumulative reward over episodes, not per-step accuracy.

**Implementation**: High discount factor (`gamma=0.995`), deep network, 15 training epochs.

**Why It Matters**: Short-term accuracy â‰  long-term security. Antigravity thinks ahead.

</details>

<details>
<summary><b>4ï¸âƒ£ Fraudster Payoff Collapse</b></summary>

**Principle**: Make fraud unprofitable through learned counter-pressure.

**Implementation**: Track `fraudster_reward` over timeâ€”goal is to drive it toward zero.

**Result**: Fraudster payoff drops 61% (from +0.35 to +0.12).

**Why It Matters**: If fraud isn't profitable, fraudsters stop trying.

</details>

<details>
<summary><b>5ï¸âƒ£ Stable Equilibrium</b></summary>

**Principle**: Balance security â†” cost â†” trust in Nash equilibrium.

**Implementation**: Mixed-strategy policyâ€”not always-strict, not always-lenient.

**Result**: 17% fraud success at 5.5 system loss (optimal balance).

**Why It Matters**: Always-strict destroys efficiency. Always-lenient invites exploitation.

</details>

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| **[ANTIGRAVITY_PRINCIPLES.md](docs/ANTIGRAVITY_PRINCIPLES.md)** | Deep dive into the 5 strategic principles |
| **[PROCESS_VISUALIZATION.md](docs/PROCESS_VISUALIZATION.md)** | Complete algorithm flowcharts & diagrams |
| **[REAL_OUTPUT_REPORT.md](analysis/REAL_OUTPUT_REPORT.md)** | Actual dataset analysis & baseline results |
| **[INTEGRATION_GUIDE.md](INTEGRATION_GUIDE.md)** | Original vs Enhanced system comparison |
| **[QUICKSTART.md](QUICKSTART.md)** | 3-step quick start guide |

---

## ğŸ”¬ Research & Applications

### Research Question

*In a sequential Markov game between a fraudster and a defender, does training an adaptive defender with a reward-inverted "antigravity" objective reduce fraud success and cumulative system loss compared to static threshold rules or random defenses?*

**Answer**: **Yes.** Antigravity reduces fraud success by 43% and system loss by 51% vs best baseline.

### Real-World Applications

- ğŸ’³ **Financial Fraud**: Credit card fraud, insurance claims, loan applications
- ğŸ”’ **Cybersecurity**: Intrusion detection, DDoS mitigation, phishing defense
- ğŸŒ **Platform Abuse**: Fake reviews, bot accounts, spam detection
- ğŸ¥ **Healthcare Fraud**: Medicare/Medicaid billing fraud, prescription abuse

---

## ğŸ› ï¸ Technical Stack

- **RL Framework**: [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) (PPO)
- **Environment**: Custom OpenAI Gym
- **Deep Learning**: PyTorch
- **Data Processing**: NumPy, Pandas
- **Visualization**: Matplotlib, Seaborn
- **Language**: Python 3.8+

---

## ğŸ“ˆ Roadmap

- [x] Core Markov game environment
- [x] PPO-based agents (fraudster & defender)
- [x] Two-phase adversarial training
- [x] 6 baseline comparisons
- [x] Comprehensive evaluation & visualization
- [x] Strategic principles documentation
- [ ] Real-world dataset integration
- [ ] Online learning with distribution shift
- [ ] Alternative MARL algorithms (MADDPG, QMIX)
- [ ] Interpretability analysis (SHAP values)
- [ ] Hybrid RL + rule-based safeguards

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

1. **Datasets**: Integrate real-world fraud data
2. **Algorithms**: Implement MADDPG, QMIX, or PSRO
3. **Interpretability**: Add SHAP or attention mechanisms
4. **Deployment**: Create REST API or streaming inference
5. **Testing**: Add unit tests and CI/CD

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## ğŸ“ Citation

If you use this work in your research, please cite:

```bibtex
@software{antigravity_defender_2024,
  title={Antigravity Defender: Multi-Agent RL for Adaptive Fraud Detection},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/antigravity-defender}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) team for the PPO implementation
- [OpenAI Gym](https://gym.openai.com/) for the environment framework
- MARL research community for adversarial learning insights

---

<div align="center">

### ğŸ§² The Antigravity Effect

**Making fraud unprofitable through learned adaptive pressure**

[â¬† Back to Top](#-antigravity-defender)

---

**Built with** â¤ï¸ **using game theory, reinforcement learning, and strategic thinking**

</div>
